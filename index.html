<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <br>
    <!-- <div class="logo" style="text-align: center;">
        <a href="index.html">
          <img src="./assets/images/logo.webp" width="10%">
        </a>
    </div> -->
    <title>CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title" style="font-size: 2.5rem;">CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion</h1>
                    <h3 class="title is-4 conference-authors"><a target="_blank" href="https://cvpr.thecvf.com/">ACL 2024</a>
                    </h3>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://scholar.google.com/citations?user=UZKS9MQAAAAJ&hl=en">Qibing&#160;Ren</a><sup>1*</sup>,
                <a target="_blank" href="https://gao-xiao-bai.github.io">Chang&#160;Gao</a><sup>2*</sup>,
                <a target="_blank" href="https://amandajshao.github.io/">Jing&#160;Shao</a><sup>3&#9993</sup>
                <a target="_blank" href="https://thinklab.sjtu.edu.cn">Junchi&#160;Yan</a><sup>1</sup>
                <a target="_blank" href="https://tanxincs.github.io">Xin&#160;Tan</a><sup>4</sup>
                <a target="_blank" href="https://scholar.google.com/citations?user=ewA4NAcAAAAJ&hl=en">Wai&#160;Lam</a><sup>2</sup>
                <a target="_blank" href="https://scholar.google.com/citations?user=yd58y_0AAAAJ&hl=en">Lizhuang&#160;Ma</a><sup>1&#9993</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University; </span>
                        <span class="author-block"><sup>2</sup>The Chinese University of Hong Kong; </span>
                        <span class="author-block"><sup>3</sup>Shanghai Artificial Intelligence Laboratory; </span>
                        <span class="author-block"><sup>4</sup>East China Normal University; </span>
                    </div>


                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>*&#160;</sup>Equal Contribution&#160;&#160;</span>
                        <span class="author-block"><sup>&#9993&#160;</sup>Corresponding author&#160;&#160;</span>
                        <!-- <span class="author-block"><sup>&dagger;&#160;</sup>Project Leader&#160;&#160;</span> -->
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2403.07865"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                            <span class="link-block">
                <a target="_blank" href="assets/2403.07865v4.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/renqibing/CodeAttack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://github.com/renqibing/CodeAttack/tree/main/data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <span class="link-block">
                <a target="_blank" href="https://mp.weixin.qq.com/s/w5dNlKw2mcBjafmR5hg8iQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-book"></i>
                  </span>
                  <span>WeChat Blog</span>
                </a>
              </span>
              
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<div class="columns is-centered has-text-centered">
    <div class="column">
        <img src="assets/images/overview.png" width="60%">
    </div>
</div>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Introduction</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        In this work, we reveal an universal safety generalization issue of current large language models (LLMs) in the domain of code. 
                        Our proposed CodeAttack achieves more than 80% attack success rate on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series.
                        We give our hypotheses about the success of CodeAttack: the misaligned bias acquired by LLMs during code training, prioritizing code completion over avoiding the potential safety risk.
                        We hope that sharing our discoveries will inspire further research into designing more robust safety alignment algorithms, towards the safer integration of LLMs into the real world.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<!--Model-->
<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">CodeAttack can breach the safety guardrails of current SOTA LLMs</span></h2>
                    <div class="columns is-vcentered">
                        <div class="column has-text-centered is-10"> <!-- Adjust the column size -->
                            <img src="assets/images/main_res.png" class="interpolation-image" width="100%"  
                                 alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <!-- Optional descriptive text can be uncommented and used here -->
                        </div>
                        <div class="column has-text-centered is-4"> <!-- Adjust the column size -->
                            <img src="assets/images/datastruc.png" class="interpolation-image" width="100%" 
                                 alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <!-- Optional descriptive text can be uncommented and used here -->
                        </div>
                    </div>
                    

                    <!-- <img src="assets/images/probing.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br> -->
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Larger distribution gap between CodeAttack and natural language leads to weaker safety generalization.</span>
                        <ul style="list-style-type: disc; padding-left: 20px;">
                            <li>We find that LLMs are more likely to exhibit unsafe behavior when <span style="color: orange; font-weight: bold;">the encoded input is less similar to natural language</span>, i.e., further from the safety training data distribution.</li>
                        </ul>
                    </span>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">A more powerful model does not necessarily lead to better safety behavior.</span>
                        <ul style="list-style-type: disc; padding-left: 20px;">
                            <li>We find that bigger models like Claude-2 and GPT-4 are still vulnerable to CodeAttack. Furthermore, CodeLlama-70b-instruct, fine-tuned on Llama-2-70b and with superior coding capabilities, <span style="color: orange; font-weight: bold;">exhibits even greater vulnerability than Llama-2-70b</span>.</li>
                        </ul>
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Effect of programming languages and pre-training bias on CodeAttack</span></h2>
                    <div class="columns is-vcentered">
                        <div class="column has-text-centered">
                            <img src="assets/images/program.png" class="interpolation-image" width="85%"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <!-- <p style="font-size: 90%; font-weight: bold;">Constructing steering vectors from the pre-training checkpoints <br>and intervening in the SFT model</p> -->
                        </div>
                        <div class="column has-text-centered">
                            <img src="assets/images/quicksort.png" class="interpolation-image" width="70%"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <!-- <p style="font-size: 90%; font-weight: bold;">Performance of various models across four general capabilities <br>and five trustworthiness capabilities</p> -->
                        </div>
                    </div>

                    <!-- <img src="assets/images/probing.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br> -->
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">The imbalanced distribution of programming languages in the code training corpus further widens the safety generalization gap.</span>
                        <ul style="list-style-type: disc; padding-left: 20px;">
                            <li>We find that LLMs’ safety behavior generalizes <span style="color: orange; font-weight: bold;">less effectively to less popular programming languages</span>. For example, using Go instead of Python increases the attack success rate of Claude-2 from 24% to 74%.</li>
                        </ul>
                    </span>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Hypotheses about the success of CodeAttack: the misaligned bias acquired during code training.</span>
                        <ul style="list-style-type: disc; padding-left: 20px;">
                            <li>We hypothesize that the learned bias makes LLMs prioritize code completion over avoiding the potential safety risk. By pretending a benign code snippet into our prompt, we find that models are more conducive to giving harmful codes.</li>
                        </ul>
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
                        In this study, we uncover generalization issues in the safety mechanisms of large language models (LLMs) when faced with novel scenarios, such as code.
We introduce CodeAttack, a novel framework that reformulates the text completion task as a code completion task. 
Our findings emphasize the importance of comprehensive red-teaming evaluations to assess the safety alignment of LLMs in long-tail distribution. 
Moreover, CodeAttack is cost-efficient and automated, eliminating the need for attackers to have domain-specific knowledge of code, suggesting a potential increase in misuse of LLMs in the code domain. 
We strongly advocate for further research into developing more robust safety alignment techniques that can generalize to unseen domains.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<!-- <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{jiang2023vima,
  title     = {VIMA: General Robot Manipulation with Multimodal Prompts},
  author    = {Yunfan Jiang and Agrim Gupta and Zichen Zhang and Guanzhi Wang and Yongqiang Dou and Yanjun Chen and Li Fei-Fei and Anima Anandkumar and Yuke Zhu and Linxi Fan},
  booktitle = {Fortieth International Conference on Machine Learning},
  year      = {2023}
}</code></pre>
    </div>
</section> -->

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{ren2024codeattack,
    title={CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion},
    author={Ren, Qibing and Gao, Chang and Shao, Jing and Yan, Junchi and Tan, Xin and Lam, Wai and Ma, Lizhuang},
    journal={arXiv preprint arXiv:2403.07865},
    year={2024}
  }</code></pre>
    </div>
</section>


</body>
</html>

